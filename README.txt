Защита от эксплуатации переноса атак на модели машинного обучения
В данной работе было построено 5 основных моделей :
1.'source_.h5' - "первоначальная модель", которой располагает злоумышленник
2.'target_fgsm.h5' - "целевая модель", построенная с использованием аугментации данных, регуляризации и дообученная FGSM
3 'target_pgd.h5'- "целевая модель", построенная с использованием аугментации данных, регуляризации и дообученная PGD
4.'cat_vs_dog.h5' - "целевая модель", обученная на другом наборе данных
5.'cat_vs_dog_pgd.h5' - "целевая модель", обученная на другом наборе данных, дообученная при помощи PGD
Была рассмотрена переносимость состязательных изображений, полученных для "первоначальной модели" злоумышленника, на "целевые модели"

Порядок выполнения работы:
1. Построение модели злоумышленника:
	1.1 Файл 'source.py' - Создание "первоначальной модели", которой предположительно обладает злоумышленник и на которой он отрабатывает атаки.

2. Построение защищенной "целевой модели", путем улучшения "первоначальной модели", получаемой в 'source.py'
	2.1.'augment_and_regular.py' - Улучшение модели, путем добавления аугментированных данных и введения регуляризации.
	2.2 Дообучение модели:
		2.2.а 'target_FGSM.py' - Состязательное дообучение модели, полученной в 'augment_and_regular.py', с помощью FGSM
		2.2.б 'pgd_target.py' -  Состязательное дообучение модели, полученной в 'augment_and_regular.py', с помощью PGD

3. Построение "целевой модели", обученной на другом наборе данных:
	3.1 'cat_vs_dog.py'- Код для создания модели
	3.2 'cat_vs_dog_adver.py' - Состязательное дообучение модели.

4. Проверка устойчивости моделей:
	4.1 Создание состязательных примеров
		4.1.а 'test_source_fgsm.py'- Генерация состязательных изображений с помощью FGSM и их сохранение
		4.1.б 'pgd_attack.py'- Генерация состязательных изображений с помощью PGD и их сохранение
	4.2 'just_test.py' - Тестирование созданных "целевых моделей" на состязательных примерах